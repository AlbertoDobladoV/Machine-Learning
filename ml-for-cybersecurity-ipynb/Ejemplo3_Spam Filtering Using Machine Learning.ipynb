{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Práctico: Detección de SPAM\n",
    "\n",
    "## Conjunto de datos: Detección de SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos está formado por diversos emails, de los cuales disponemos tanto del texto, asunto, correo de emisores, receptores y fechas de emisión.\n",
    "\n",
    "Disponemos de dos bases de datos, una está compuesta sólo de correos considerados spam, la segunda de correos considerados buenos."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A continuación vamos a establecer los directorios de lectura de los dos bloques de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "spam_emails_path = os.path.join(\"spamassassin-public-corpus\", \"spam\")\n",
    "ham_emails_path = os.path.join(\"spamassassin-public-corpus\", \"ham\")\n",
    "labeled_file_directories = [(spam_emails_path, 0), (ham_emails_path, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos los documentos listados extrayendo el texto que será analizado y procesado para extraer información del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_corpus = []\n",
    "labels = []\n",
    "\n",
    "for class_files, label in labeled_file_directories:\n",
    "    files = os.listdir(class_files)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(class_files, file)\n",
    "        try:\n",
    "            with open(file_path, \"r\") as currentFile:\n",
    "                email_content = currentFile.read().replace(\"\\n\", \"\")\n",
    "                email_content = str(email_content)\n",
    "                email_corpus.append(email_content)\n",
    "                labels.append(label)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División del conjunto de dato en train y test\n",
    "Aplicamos la técnica aprendida de división en train y test que reduce la posibilidad de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    email_corpus, labels, test_size=0.2, random_state=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de datos, convertimos texto en vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se ha aplicado la técnica de vectorizado por hashing.\n",
    "Y posteriormente tfidf para buscar similitudes entre palabras en función de sus palabras vecinas en frases y párrafos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo de árbol de decisión\n",
    "Se trata de un modelo de clasificació binaria en el que las clases están balanceadas.\n",
    "Se consideran las clases balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 HashingVectorizer(alternate_sign=True, analyzer='word',\n",
       "                                   binary=False, decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, n_features=1048576,\n",
       "                                   ngram_range=(1, 3), norm='l2',\n",
       "                                   preprocessor=None, stop_words=None,\n",
       "                                   strip_accents=None,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None...\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('dt',\n",
       "                 DecisionTreeClassifier(class_weight='balanced',\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features=None, max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn import tree\n",
    "\n",
    "nlp_followed_by_dt = Pipeline(\n",
    "    [\n",
    "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1, 3))),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=True,)),\n",
    "        (\"dt\", tree.DecisionTreeClassifier(class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "nlp_followed_by_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(nlp_followed_by_dt.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_test_pred = nlp_followed_by_dt.predict(X_test)\n",
    "print(\"Acuracy\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Matriz de confusión\", confusion_matrix(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
